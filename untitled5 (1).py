# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GUxs_bj78baL9L7xDB0v1ADSGc_SxkyN
"""

!pip install transformers[sentencepiece]  sacrebleu -q

import os

!pip install --upgrade --force-reinstall tensorflow

!pip install numpy==1.24.4

import numpy as np
import tensorflow as tf
import sys
import transformers
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import AdamWeightDecay
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_checkpoint = "Helsinki-NLP/opus-mt-en-ur"

!pip install --upgrade datasets fsspec -q

!datasets-cli delete-cache cfilt/iitb-english-hindi

pip install --upgrade datasets

pip install -U datasets

pip install fsspec==2023.9.2

from datasets import load_dataset

raw_datasets = load_dataset("Mudasir692/english-urdu")

raw_datasets

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer("This is english sentence and I want to translate in urdu sentence")

with tokenizer.as_target_tokenizer():
  print(tokenizer("This is sentence"))

def is_valid(example):
    return example["English"] is not None and example["Urdu"] is not None

raw_datasets = raw_datasets.filter(is_valid)

max_input_length = 128
max_target_length = 128

source_lang = "English"
target_lang = "Urdu"

def preprocess_function(examples):
  inputs = examples[source_lang]
  targets = examples[target_lang]
  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

  with tokenizer.as_target_tokenizer():
    labels = tokenizer(targets, max_length=max_target_length, truncation=True)

  model_inputs["labels"]= labels["input_ids"]
  return model_inputs

preprocess_function(raw_datasets["train"][:2])

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

batch_size = 16
learning_rate = 2e-5
weight_decay = 0.01
num_train_epochs = 1 #low because high can take more time, I am now learning so I just try for 1 epoch

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")

generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128)

train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["test"],    # training data is too large maybe take 10-12 hours that is why I use test samples only for quick results
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator,
)

optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)
model.compile(optimizer=optimizer)

model.fit(train_dataset, epochs= 1)

model.save_pretrained("/content/drive/MyDrive/tf_model2/")



from google.colab import drive
drive.mount('/content/drive')

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForSeq2SeqLM.from_pretrained("/content/drive/MyDrive/tf_model2/")

input_text = "Tell me about Pakistan"
tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=128)
print(tokenizer.decode(out[0], skip_special_tokens=True))

