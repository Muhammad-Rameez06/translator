# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YwO4X0tTVxAvPIbI2U9M2BGs1ETy5eMF
"""

!pip install transformers[sentencepiece]  sacrebleu -q

import os

!pip install --upgrade --force-reinstall tensorflow

import numpy as np
import tensorflow as tf
import sys
import transformers
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import AdamWeightDecay
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_checkpoint = "Helsinki-NLP/opus-mt-en-ur"

pip install --upgrade datasets

from datasets import load_dataset

raw_datasets = load_dataset("Mudasir692/english-urdu")

raw_datasets

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer("This is english sentence and I want to translate in urdu sentence")

with tokenizer.as_target_tokenizer():
  print(tokenizer("This is sentence"))

def is_valid(example):
    return example["English"] is not None and example["Urdu"] is not None

raw_datasets = raw_datasets.filter(is_valid)

max_input_length = 512
max_target_length = 512

source_lang = "English"
target_lang = "Urdu"

def preprocess_function(examples):
  inputs = examples[source_lang]
  targets = examples[target_lang]
  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

  with tokenizer.as_target_tokenizer():
    labels = tokenizer(targets, max_length=max_target_length, truncation=True)

  model_inputs["labels"]= labels["input_ids"]
  return model_inputs

preprocess_function(raw_datasets["train"][:2])

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

batch_size = 16
learning_rate = 2e-5
weight_decay = 0.01
num_train_epochs = 3 #low because high can take more time, I am now learning so I just try for 3 epoch

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")

generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128)

train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],    # training data is too large maybe take 10-12 hours that is why I use test samples only for quick results
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator,
)

optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)
model.compile(optimizer=optimizer)

model.fit(train_dataset, epochs= 2)

model.save_pretrained("/content/drive/MyDrive/tf_model4/")
tokenizer.save_pretrained("/content/drive/MyDrive/tf_model4/")

from google.colab import drive
drive.mount('/content/drive')

input_text = "The rapid advancement of technology continues to shape every aspect of our lives, from how we communicate to how we work and learn. Artificial intelligence, in particular, has revolutionized industries by enabling machines to perform tasks once limited to humans. From virtual assistants and facial recognition to medical diagnostics and autonomous vehicles, AI's presence is growing rapidly. However, with this progress come ethical concerns, including data privacy, bias in algorithms, and job displacement. Addressing these issues requires collaboration between developers, policymakers, and society at large. Education and awareness play a crucial role in ensuring that technological growth benefits everyone. By fostering innovation responsibly, we can harness the power of AI to improve quality of life globally."
tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=512)
print(tokenizer.decode(out[0], skip_special_tokens=True))

